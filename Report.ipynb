{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **1. Introduction**\n## **1.1. dCache**\n   First of all, it worths saying some words about the system we are going to work with -  dCache. Built in Java dCache is a distributed mass-storage system that allows us to manage huge ammount of scientific data. The data are distributed among the large number of heterogenous pools(nodes) that handle with data storage and transfer. A client can easily get access to dCache data through requests.  \n   \n   Information we are intersted in is about transactions occurred in dCache. It is contained in *billing* files which are a set of JSON dictionaries for a particular date. There are several main types of transactions: requset, transfer, remove, store, restore, but  only stores will be in our sphere of interests (later will be explained why).\n\n   Since amount of data to process is huge enough, to avoid overloading of our local machines we use unified analytics engine for large-scale data processing - **Apache Spark**.\n   \n   Logistic regression: https://spark.apache.org/docs/latest/mllib-linear-methods.html#logistic-regression; https://www.pdfdrive.com/applied-logistic-regression-e172207141.html\n   ","metadata":{}},{"cell_type":"markdown","source":"## **1.2. Apache Spark**","metadata":{}},{"cell_type":"markdown","source":"**Apache Spark** is an open-source, distributed processing system used for big data workloads. It utilizes in-memory caching, and optimized query execution for fast analytic queries against data of any size.\n\nSpark was introduced in 2012. Before Spark, Hadoop MapReduce was commonly used for big data analytics. \n\nHadoop MapReduce processes big datasets with a parallel, distributed algorithm. However, a challenge in using MapReduce is the sequential multi-step process it takes to run a job. With each step requiring a disk read, and write, MapReduce jobs are slower due to the latency of disk I/O.\n\nTo overcome this problem, Spark was created. It achieves this by processing in-memory, reducing the number of steps in a job, and by reusing data across multiple parallel operations. It is accomplished through the creation of DataFrames, an abstraction over Resilient Distributed Dataset (RDD), which is a collection of objects that is cached in memory, and reused in multiple Spark operations.","metadata":{}},{"cell_type":"markdown","source":"## **1.3. Machine Learning**","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"\n","metadata":{}},{"cell_type":"markdown","source":"# **2. Analysis**\n## **2.1. Motivation**\nAs in each system, breakdowns and errors sometimes occur in dCahce. Few people would dispute that it is important to detect them automatically and warn users afterward. Imagine the situation, a scientist from Japan failed to access data from the node placed in the mid-Europian region, and now he is trying to figure out if there was a problem with his local machine or with the entire system, but he can not get any information, because due to the difference in time zones local office in Europe is closed already. In such cases, it is especially important to warn the user if something was wrong. That is why we decided to develop a machine learning model, helping to determine undesirable situations.\n","metadata":{}},{"cell_type":"markdown","source":"## **2.2. Data Pre-Selection**\nAs was said before, we were interested only *store* type of transactions. There are two main reasons for it. First - structure of messages with type 'store' is not really complicated, there are not many features to analyze in comparison with 'transfer', for instance. Second - there are sufficient number of instances with message type 'store' in dCache.\n\nTwo random days were chosen for analysis: 2021-07-10 and 2021-08-01. For converting data to RDD we wrote special function \"convert_data\". As you may see, there are two parametrs in it: *file* - a file's directory, *msgType* - type pf a message.","metadata":{}},{"cell_type":"code","source":"def convert_data(file, msgType):\n    data = sc.textFile(file)\n    billing = data.map(lambda row: json.loads(row)).filter(lambda row: row.get('msgType',None) == msgType)\n    return billing","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To combine data from both days SparkContext method *union()* were used:","metadata":{}},{"cell_type":"code","source":"billing_RDD = sc.union(\n    [\n        convert_data('/pnfs/desy.de/desy/dcache-operations/billing-archive/xfel/2021/07/billing-2021-07-10.json',\"store\"),\n        convert_data('/pnfs/desy.de/desy/dcache-operations/billing-archive/xfel/2021/08/billing-2021-08-01.json',\"store\")\n    ]\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **2.5. Algorithm selection**\nSince our problem is a classification problem, we should use classifiers. To simplify process a bit we decided to choose supervise learning. As a machine learning library we decided to choose \"Mllib\". ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}