{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **1. Introduction**\n## **1.1. dCache**\n   First of all, it worths saying some words about the system we are going to work with -  dCache. Built in Java dCache is a distributed mass-storage system that allows us to manage huge ammount of scientific data. The data are distributed among the large number of heterogenous pools(nodes) that handle with data storage and transfer. A client can easily get access to dCache data through requests.  \n   \n   Information we are intersted in is about transactions occurred in dCache. It is contained in *billing* files which are a set of JSON dictionaries for a particular date. There are several main types of transactions: requset, transfer, remove, store, restore, but  only stores will be in our sphere of interests (later will be explained why).\n\n   Since amount of data to process is huge enough, to avoid overloading of our local machines we use unified analytics engine for large-scale data processing - **Apache Spark**.\n   \n   Logistic regression: https://spark.apache.org/docs/latest/mllib-linear-methods.html#logistic-regression; https://www.pdfdrive.com/applied-logistic-regression-e172207141.html\n   ","metadata":{}},{"cell_type":"markdown","source":"## **1.2. Apache Spark**","metadata":{}},{"cell_type":"markdown","source":"**Apache Spark** is an open-source, distributed processing system used for big data workloads. It utilizes in-memory caching, and optimized query execution for fast analytic queries against data of any size. [1.]\n\nSpark was introduced in 2012. Before Spark, Hadoop MapReduce was commonly used for big data analytics. \n\nHadoop MapReduce processes big datasets with a parallel, distributed algorithm. However, a challenge in using MapReduce is the sequential multi-step process it takes to run a job. With each step requiring a disk read, and write, MapReduce jobs are slower due to the latency of disk I/O.\n\nTo overcome this problem, Spark was created. It achieves this by processing in-memory, reducing the number of steps in a job, and by reusing data across multiple parallel operations. It is accomplished through the creation of DataFrames, an abstraction over Resilient Distributed Dataset (RDD), which is a collection of objects that is cached in memory, and reused in multiple Spark operations.","metadata":{}},{"cell_type":"markdown","source":"## **1.3. Machine Learning**","metadata":{}},{"cell_type":"markdown","source":"**Machine learning(ML)** is a subfield of artificial intelligence, which is broadly defined as the capability of a machine to imitate intelligent human behavior. Artificial intelligence systems are used to perform complex tasks in a way that is similar to how humans solve problems.  [2.]","metadata":{}},{"cell_type":"markdown","source":"Broadly, there are mainly 3 types of ML algorithms:\n\n* Supervised Learning: When an algorithm learns from example data and associated target responses that can consist of numeric values or string labels, such as classes or tags, in order to later predict the correct response when posed with new examples comes under the category of Supervised learning.\n\n* Unsupervised learning: When an algorithm learns from plain examples without any associated response, leaving to the algorithm to determine the data patterns on its own.\n\n* Reinforcement learning: When you present the algorithm with examples that lack labels, as in unsupervised learning. However, you can accompany an example with positive or negative feedback according to the solution so that the algorithm makes its own decisions, and the decisions bear consequences.\n\n<a href=\"https://ibb.co/sH5Nv38\"><img src=\"https://i.ibb.co/YZD17jH/figure01.png\" alt=\"figure01\" border=\"0\"></a>","metadata":{}},{"cell_type":"markdown","source":"# **2. Analysis**\n## **2.1. Motivation**\nAs in each system, breakdowns and errors sometimes occur in dCahce. Few people would dispute that it is important to detect them automatically and warn users afterward. Imagine the situation, a scientist from Japan failed to access data from the node placed in the mid-Europian region, and now he is trying to figure out if there was a problem with his local machine or with the entire system, but he can not get any information, because due to the difference in time zones local office in Europe is closed already. In such cases, it is especially important to warn the user if something was wrong. That is why we decided to develop a machine learning model, helping to determine undesirable situations.\n","metadata":{}},{"cell_type":"markdown","source":"## **2.2. Importing Libraries and setting up the Spark Configuration**","metadata":{}},{"cell_type":"code","source":"# Include the codes for importing Libraries","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **2.3. Data Pre-Selection**\nAs was said before, we were interested only *store* type of transactions. There are two main reasons for it. First - structure of messages with type 'store' is not really complicated, there are not many features to analyze in comparison with 'transfer', for instance. Second - there are sufficient number of instances with message type 'store' in dCache.\n\nTwo random days were chosen for analysis: 2021-07-10 and 2021-08-01. For converting data to RDD we wrote special function \"convert_data\". As you may see, there are two parametrs in it: *file* - a file's directory, *msgType* - type pf a message.","metadata":{}},{"cell_type":"code","source":"def convert_data(file, msgType):\n    data = sc.textFile(file)\n    billing = data.map(lambda row: json.loads(row)).filter(lambda row: row.get('msgType',None) == msgType)\n    return billing","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To combine data from both days SparkContext method *union()* were used:","metadata":{}},{"cell_type":"code","source":"billing_RDD = sc.union(\n    [\n        convert_data('/pnfs/desy.de/desy/dcache-operations/billing-archive/xfel/2021/07/billing-2021-07-10.json',\"store\"),\n        convert_data('/pnfs/desy.de/desy/dcache-operations/billing-archive/xfel/2021/08/billing-2021-08-01.json',\"store\")\n    ]\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **2.4. Feature Description**","metadata":{}},{"cell_type":"markdown","source":"After selecting the msgType = \"Store\", we can see that the RDD has many columns of data. \n\nAll the columns along with an example, can be seen using the following code.","metadata":{}},{"cell_type":"code","source":"# Put the msgtype structure code in here","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"However, all this columns are not required for our ML analysis. So, we have selected few columns which we deemed important for our task of Anomaly Detection.\n\nAll the other columns were rejected because they were unique label to each event and thus wouldn't provide much insight into the Anomaly Detection algorithm that we are trying to construct.","metadata":{}},{"cell_type":"markdown","source":"| Features | Description |\n|----------|-------------|\n| CellName |             |\n|          |             |\n|          |             |","metadata":{}},{"cell_type":"markdown","source":"## **2.5. Data Pre-Processing**","metadata":{}},{"cell_type":"markdown","source":"Before the data can be used for our ML purposes it has to be transformed so that they are suitable for pplying our ML algorithm.","metadata":{}},{"cell_type":"markdown","source":"We use some wrapper functions to do some initial transformations.","metadata":{}},{"cell_type":"markdown","source":"* Queueing Time - No modification ","metadata":{}},{"cell_type":"code","source":"# Put the code for wrapper functions","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have one column `CellName` which has 'String' type. Since, a 'String' type column can't directly be used for any ML analysis, we have to convert it into a suitable form.\n\nWe use the `StringIndexer` module in Mllib followed by the `OneHotEncoder` module, which transforms the CellName columns into a sparse matrix which can then be used for ML analysis.","metadata":{}},{"cell_type":"code","source":"# codes for String Indexer .....etc","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To be able to apply Logistic Regression on our data we have to convert the DataFrame into a single Vector which is achieved by the `Vectorizer` module in Mllib.","metadata":{}},{"cell_type":"code","source":"# Code for vectorizer","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## **2.5. Algorithm selection**\nSince our problem is a classification problem, we should use classifiers. To simplify process a bit we decided to choose supervise learning. As a machine learning library we decided to choose \"Mllib\". ","metadata":{}},{"cell_type":"markdown","source":"# **4. References**","metadata":{"execution":{"iopub.status.busy":"2021-09-05T11:12:57.481187Z","iopub.execute_input":"2021-09-05T11:12:57.481636Z","iopub.status.idle":"2021-09-05T11:12:57.486368Z","shell.execute_reply.started":"2021-09-05T11:12:57.481538Z","shell.execute_reply":"2021-09-05T11:12:57.485384Z"}}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}