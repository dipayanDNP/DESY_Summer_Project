{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Motivation**\nAs in each system, breakdowns and errors sometimes occur in **dCahce**. Few people would dispute that it is important to detect them automatically and warn users afterward. Imagine the situation, a scientist from Japan failed to access data from the node placed in the mid-Europian region, and now he is trying to figure out if there was a problem with his local machine or with the entire system, but he can not get any information, because due to the difference in time zones local office in Europe is closed already. In such cases, it is especially important to warn the user if something was wrong. That is why we decided to develop a **machine learning** model, helping to determine undesirable situations.\n  \nThis report is devoted to our solution for this problem. We have prepared the model, analysing information about dCache transactions and  distinguishing `signal` - when the system works fine, and `backgorund` - when something goes wrong. You can find a more detailed description on the following pages of the report.\n ","metadata":{}},{"cell_type":"markdown","source":"# **1. Introduction**\n## **1.1. dCache**\n   First of all, it worths saying some words about the system we are going to work with -  dCache. Built in Java dCache is a distributed mass-storage system that allows us to manage huge ammount of scientific data. The data are distributed among the large number of heterogenous pools(nodes) that handle with data storage and transfer. A client can easily get access to dCache data through requests.  \n   \n   Information we are intersted in is about transactions occurred in dCache. It is contained in *billing* files which are a set of JSON dictionaries for a particular date. There are several main types of transactions: requset, transfer, remove, store, restore, but  only `stores` will be in our sphere of interests (later will be explained why).\n\n   Since amount of data to process is huge enough, to avoid overloading of our local machines we use unified analytics engine for large-scale data processing - **Apache Spark**.\n   ","metadata":{}},{"cell_type":"markdown","source":"## **1.2. Apache Spark**","metadata":{}},{"cell_type":"markdown","source":"**Apache Spark** is an open-source, distributed processing system used for big data workloads. It utilizes in-memory caching, and optimized query execution for fast analytic queries against data of any size. [1.]\n\nSpark was introduced in 2012. Before Spark, Hadoop MapReduce was commonly used for big data analytics. \n\nHadoop MapReduce processes big datasets with a parallel, distributed algorithm. However, a challenge in using MapReduce is the sequential multi-step process it takes to run a job. With each step requiring a disk read, and write, MapReduce jobs are slower due to the latency of disk I/O.\n\nTo overcome this problem, Spark was created. It achieves this by processing in-memory, reducing the number of steps in a job, and by reusing data across multiple parallel operations. It is accomplished through the creation of DataFrames, an abstraction over Resilient Distributed Dataset (RDD), which is a collection of objects that is cached in memory, and reused in multiple Spark operations.","metadata":{}},{"cell_type":"markdown","source":"## **1.3. Machine Learning**","metadata":{}},{"cell_type":"markdown","source":"**Machine learning(ML)** is a subfield of artificial intelligence, which is broadly defined as the capability of a machine to imitate intelligent human behavior. Artificial intelligence systems are used to perform complex tasks in a way that is similar to how humans solve problems.  [2.]","metadata":{}},{"cell_type":"markdown","source":"Broadly, there are mainly 3 types of ML algorithms:\n\n* Supervised Learning: When an algorithm learns from example data and associated target responses that can consist of numeric values or string labels, such as classes or tags, in order to later predict the correct response when posed with new examples comes under the category of Supervised learning.\n\n* Unsupervised learning: When an algorithm learns from plain examples without any associated response, leaving to the algorithm to determine the data patterns on its own.\n\n* Reinforcement learning: When you present the algorithm with examples that lack labels, as in unsupervised learning. However, you can accompany an example with positive or negative feedback according to the solution so that the algorithm makes its own decisions, and the decisions bear consequences.\n\n<a href=\"https://ibb.co/sH5Nv38\"><img src=\"https://i.ibb.co/YZD17jH/figure01.png\" alt=\"figure01\" border=\"0\"></a>\n\nSource: [Link](https://developer.ibm.com/articles/cc-models-machine-learning/)","metadata":{}},{"cell_type":"markdown","source":"## **1.4. Logistic Regression**","metadata":{}},{"cell_type":"markdown","source":"Logistic Reression is a type of Supervised Algorithm that is used for classification problems, i.e. correctly classifying various data points to their correct data labels. It calculates the probability of the datasample being a particular class. \n\nIn our project, we have worked with binary class Logistic Regression Model which classifies 2 Labels 0 and 1.\n\nThe conditional probability that our Logistic model gives a particular class given the dataset is given by -\n\n<a href=\"https://ibb.co/vXSL8Xr\"><img src=\"https://i.ibb.co/XYqFTYd/1-I0l-W7-Ydv-Tn3m-HXh56p-Yx-ZQ.gif\" alt=\"1-I0l-W7-Ydv-Tn3m-HXh56p-Yx-ZQ\" border=\"0\"></a>","metadata":{}},{"cell_type":"markdown","source":"where\n\n* w = weight values which are determined by our ML algorithm\n\n* x = data input\n\n* y = conditional probability of predicting a particular class given the dataset","metadata":{}},{"cell_type":"markdown","source":"Plot of y with a one dimensional data input x will have the following form - \n\n<a href=\"https://ibb.co/sg1spY7\"><img src=\"https://i.ibb.co/GdMCfSm/1-Un-SW1b5-Ldp-Fl-Bx5h-R54-J0w.png\" alt=\"1-Un-SW1b5-Ldp-Fl-Bx5h-R54-J0w\" border=\"0\"></a>\n\nSource: [Link](https://towardsdatascience.com/an-introduction-to-logistic-regression-8136ad65da2e)","metadata":{}},{"cell_type":"markdown","source":"# **2. Analysis**\n## **2.1. Importing Libraries and setting up the Spark Configuration**\nThe following libraries and Spark Configurations were used:","metadata":{}},{"cell_type":"code","source":"import findspark\nfindspark.init()\nimport os\nfrom pyspark import SparkContext\nfrom pyspark import SparkConf\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql.types import *\nfrom pyspark_dist_explore import hist\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom collections import Counter\nfrom datetime import datetime\nimport time\nimport re\nfrom ipaddress import ip_address\nfrom urllib.parse import urlparse\nimport json\nimport isodate\nfrom dateutil import parser\nfrom dateutil import tz\nfrom pyspark.sql.types import StructField, StructType, StringType\nfrom pyspark.sql import Row\nfrom urllib.request import urlopen\n\nsc_conf = SparkConf().setMaster('spark://dcache-dot1.desy.de:5000').set('spark.executor.memory',\n'26G').set('spark.driver.memory','8G').set('spark.driver.maxResultSize','8G')\n#sc = SparkContext(appName=\"PythonStreaming\")\nsc = SparkContext(conf = sc_conf)\nsqlContext = SQLContext(sc)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **2.2. Data Pre-Selection**\nAs was said before, we were interested in only `store` type of transactions. There are two main reasons for it. First - structure of messages with type `store` is not really complicated, there are not many features to analyze in comparison with `transfer`, for instance. Second - there are sufficient number of instances with message type `store` in dCache.\n\nTwo random days were chosen for analysis: 2021-07-10 and 2021-08-01. For converting data to RDD we wrote special function `convert_data`. As you may see, there are two parametrs in it: `file` - a file's directory, `msgType` - type of a message.","metadata":{}},{"cell_type":"code","source":"def convert_data(file, msgType):\n    data = sc.textFile(file)\n    billing = data.map(lambda row: json.loads(row)).filter(lambda row: row.get('msgType',None) == msgType)\n    return billing","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To combine data from both days SparkContext method *union()* was used:","metadata":{}},{"cell_type":"code","source":"billing_RDD = sc.union(\n    [\n        convert_data('/pnfs/desy.de/desy/dcache-operations/billing-archive/xfel/2021/07/billing-2021-07-10.json',\"store\"),\n        convert_data('/pnfs/desy.de/desy/dcache-operations/billing-archive/xfel/2021/08/billing-2021-08-01.json',\"store\")\n    ]\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **2.3. Feature Description**","metadata":{}},{"cell_type":"markdown","source":"After selecting the msgType = \"Store\", we can see that the RDD has many columns of data. \n\nAll the columns along with an example, can be seen using the following code.","metadata":{}},{"cell_type":"code","source":"billing_RDD.first()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"However, all this columns are not required for our ML analysis. So, we have selected few columns which we deemed important for our task of Anomaly Detection.\n\nAll the other columns were rejected because they were unique label to each event and thus wouldn't provide much insight into the Anomaly Detection algorithm that we are trying to construct.","metadata":{}},{"cell_type":"markdown","source":"| Features | Description |\n|----------|-------------|\n| CellName |             |\n|          |             |\n|          |             |","metadata":{}},{"cell_type":"markdown","source":"## **2.4. Data Pre-Processing**","metadata":{}},{"cell_type":"markdown","source":"Before the data can be used for our ML purposes it has to be transformed so that they are suitable for applying our ML algorithm.","metadata":{}},{"cell_type":"markdown","source":"We use some wrapper functions to do some initial transformations.","metadata":{}},{"cell_type":"markdown","source":"* `CellName` - Taking the last 4 numbers and converting it into a string.\n\n* `date_time` - Converting the date into an unix epoch format, which is the number of seconds that have elapsed since January 1, 1970.\n\n* `fileSize` `transferTime` - Took the log of numbers\n\n* `queuingTime` - Took the log of numbers, taking care of the null or 0 cases and assigning them -10\n\n* `initial label` - Assigned all non-zero status numbers as 1 and zero as 0\n","metadata":{}},{"cell_type":"code","source":"def cellName(cellName):\n    s=cellName[len(cellName)-5:]\n    s1=s.split('-')\n    return str(s1[0])+str(s1[1])\n\ndef date_time(date):\n    return int(time.mktime(parser.parse(date).timetuple()))\n\ndef fileSize(fileSize):\n    return float(np.log(fileSize))\n\ndef queuingTime(queuingTime):\n    if (queuingTime == 0) | (queuingTime is None):\n        return -10.0\n    else:    \n        return float(np.log(queuingTime))\n    \ndef transferTime(transferTime):\n    return float(np.log(transferTime))\n\ndef initial_label(status):\n    if status['code']!=0:\n        return 1\n    else:\n        return 0","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"billing_Schema =  StructType([\n    StructField('cellName', StringType(), True),\n    StructField('date_time', LongType(), True),\n    StructField('fileSize', FloatType(), True),\n    StructField('transferTime', FloatType(), True),\n    StructField('queuingTime', FloatType(), True),\n    StructField('initial_label', LongType(), True)\n   ])\n\ndef parse_billing(entry):\n    parse_list = [ cellName(entry.get('cellName')),\n                   date_time(entry.get('date')),\n                   fileSize(entry.get('fileSize')),\n                   transferTime(entry.get('transferTime')),\n                   queuingTime(entry.get('queuingTime')),\n                   initial_label(entry.get('status'))\n                  ]\n    info = tuple( field for field in parse_list )\n    return info\n\nbilling_df = sqlContext.createDataFrame(billing_RDD.map(lambda s: parse_billing(s)), billing_Schema)\nbilling_df.createOrReplaceTempView(\"billing_desy\") #name creation for sql quiries\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"billing_df.printSchema()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have one column `cellName` which has 'String' type. Since, a 'String' type column can't directly be used for any ML analysis, we have to convert it into a suitable form.\n\nWe use the `StringIndexer` module in Mllib followed by the `OneHotEncoder` module, which transforms the CellName columns into a sparse matrix which can then be used for ML analysis.","metadata":{}},{"cell_type":"code","source":"from pyspark.ml.feature import StringIndexer, OneHotEncoder\nstr_columns=['cellName']\nstring_indexer=[ StringIndexer(inputCol=x, outputCol=x+'_StringIndexer', handleInvalid='skip') for x in str_columns]\none_hot_encoder=[OneHotEncoder(inputCol=f\"{x}_StringIndexer\", outputCol=f\"{x}_OneHotEncoder\") for x in str_columns]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To be able to apply Logistic Regression on our data we have to convert the DataFrame into a single vector of features called 'features' which is achieved by the `Vectorizer` module in Mllib.","metadata":{}},{"cell_type":"code","source":"from pyspark.ml.feature import VectorAssembler\nnum_columns=['fileSize', 'transferTime', 'queuingTime']\nassembler_input=num_columns+[f\"{x}_OneHotEncoder\" for x in str_columns]\nvector_assembler=VectorAssembler(inputCols=assembler_input, outputCol='features')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note, that `date_time` field was not included in the vector of features. That is because `date_time` is a unique feature for each file, which does not provide an ML model with important information. However, it plays a crucial role in data sampling (later will be shown how), that is why we did not exclude `date_time` from the list of fields. ","metadata":{}},{"cell_type":"markdown","source":"## **2.5. Data Sampling and Labeling**\nSince, we had 2 classes to distinguish - `signal` and `background`, we had to use a classification model for our problem.\n\nBefore starting to apply machine learning for classifictaion, it is essetial to prepare data samples and label instances correctly. \n\nFirst of all, we would like to describe labeling of the data instances. Initial approach with using `status_code` feature (look at the `initial_label` field, point 2.5) is a bit naive to make a decision if a particular instance is a part of background or signal. So, we have developed more sophisticated algorithm. New labeling algorithm includes two steps:\n* Using `date_time` field, we divide all available time (for two chosen dates) on a particular number of bins, each bin equals one hour of time. \n* Averaging values of the field `initial_label` for a particular bin, we compare output value with the threshold (threshold=0.5 in our case). If the averaged value is less than the threshold we consider all instances as signal (fill 0 as the `label` for each instance of the bin), otherwise as background (fill 1 as the the `label` for each instance of the bin).\n\nThe second issue is how to sample the data. For comparison we decided to apply two ways:\n* Extracting first 70 % instances for each hour sequently (serial way) \n* Extracting  70 % instances for each hour randomly (random way)\n\nCode examples, including labeling and two ways of the data selection, are represented below:","metadata":{}},{"cell_type":"code","source":"treshhold=0.5\nmin_val=sqlContext.sql(\"select MIN(date_time) from billing_desy\").collect()\nmin_val=np.array(min_val)[0]\nmax_val=sqlContext.sql(\"select MAX(date_time) from billing_desy\").collect()\nmax_val=np.array(max_val)[0]\nbin_num=np.round((max_val-min_val)/3600,0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"serial way\"\"\"\n\n#import lit for labeling\nfrom pyspark.sql.functions import lit\n\n#creation empty dataframe for followed union\nbilling_Schema_empty =  StructType([\n    StructField('cellName', StringType(), True),\n    StructField('date_time', LongType(), True),\n    StructField('fileSize', FloatType(), True),\n    StructField('transferTime', FloatType(), True),\n    StructField('queuingTime', FloatType(), True),\n    StructField('initial_label', LongType(), True),\n    StructField('label', IntegerType(), True)\n   ])\nserial_df=sqlContext.createDataFrame(sc.emptyRDD(), billing_Schema_empty)\n    \nfor i in range(int(bin_num)):\n    #spliting by hours\n    new_df=sqlContext.sql(\"select * from billing_desy where date_time between {} and {} order by date_time DESC\".format(\n        int(min_val)+3600*i,int(min_val)+3600*(i+1)))\n    new_avg=sqlContext.sql(\"select AVG(initial_label) from billing_desy where date_time between {} and {}\".format(\n        int(min_val)+3600*i,int(min_val)+3600*(i+1))).collect()\n    new_avg=np.array(new_avg)[0][0]\n    \n    if new_avg is None:\n        continue\n        \n    else:\n        new_avg=float(new_avg)\n        \n        #serial extracting\n        per_70=new_df.take(int(np.round(0.7*new_df.count(),0))+1)\n        per_70_df=sqlContext.createDataFrame(per_70)\n    \n    #labeling\n    if new_avg>=treshhold:\n        per_70_df=per_70_df.withColumn(\"label\", lit(1))\n    else:\n        per_70_df=per_70_df.withColumn(\"label\", lit(0))\n    serial_df=serial_df.union(per_70_df)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"serial_df.groupBy(\"label\").count().show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"random way\"\"\"\n\n#creation of empty dataframe for followed union\nbilling_Schema_empty =  StructType([\n    StructField('cellName', StringType(), True),\n    StructField('date_time', LongType(), True),\n    StructField('fileSize', FloatType(), True),\n    StructField('transferTime', FloatType(), True),\n    StructField('queuingTime', FloatType(), True),\n    StructField('initial_label', LongType(), True),\n    StructField('label', IntegerType(), True)\n   ])\nrand_df=sqlContext.createDataFrame(sc.emptyRDD(), billing_Schema_empty)\n    \nfor i in range(int(bin_num)):\n    #spliting by hours\n    new_df=sqlContext.sql(\"select * from billing_desy where date_time between {} and {} order by date_time DESC\".format(\n        int(min_val)+3600*i,int(min_val)+3600*(i+1)))\n    new_avg=sqlContext.sql(\"select AVG(initial_label) from billing_desy where date_time between {} and {}\".format(\n        int(min_val)+3600*i,int(min_val)+3600*(i+1))).collect()\n    new_avg=np.array(new_avg)[0][0]\n    \n    if new_avg is None:\n        continue\n        \n    else:\n        new_avg=float(new_avg)\n        \n        #random extracting\n        rand_70_df, rand_30_df= new_df.randomSplit([0.7,0.3],seed=7)\n        \n    #labeling\n    if new_avg>=treshhold:\n        rand_70_df=rand_70_df.withColumn(\"label\", lit(1))\n    else:\n        rand_70_df=rand_70_df.withColumn(\"label\", lit(0))\n        \n    rand_df=rand_df.union(rand_70_df)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rand_df.groupBy(\"label\").count().show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **2.6. Algorithm Selection**\nHaving prepared data, chosen samples and labeled instances, we are able to apply machine learning algorithms. To simplify things and easily get decision probabilities, we chose `Logistic Regression` as our Classifier model. We used the `Mllib` module that is availaible with spark to perform our analysis.\n\nTo combine all steps of the data pre-processing (look at point 2.5) and not to do them separately each time, we used `Pipline`. Output data of `Pipline` model are appropriate for the training of `Logistic Regression` model. Code goes below:","metadata":{}},{"cell_type":"code","source":"#splitting of selected data on train and test samples (serial_df for sequential selection is in this exaple, rand_df is also possible)\ntrain, test= serial_df.randomSplit([0.7,0.3],seed=7)\n\n#designation of stages for Pipline\nstages=[]\nstages+=string_indexer\nstages+=one_hot_encoder\nstages+=[vector_assembler]\n\n#Pipline model creation\nfrom pyspark.ml import Pipeline\npipeline=Pipeline().setStages(stages)\nmodel=pipeline.fit(train)\n\n#transforming of the train sample for Logistic Regression\nX_train=model.transform(train)\nX_train.createOrReplaceTempView(\"X_train\")\ndata=sqlContext.sql(\"select distinct(features),label from X_train\")\n\n#training of Logistic Regression model\nfrom pyspark.ml.classification import LogisticRegression\nlr=LogisticRegression().fit(data)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **3. Results**\n","metadata":{"tags":[]}},{"cell_type":"code","source":"num_columns=['fileSize', 'transferTime', 'queuingTime']\nstr_columns=['cellName']\n\nfrom pyspark.ml.feature import (StringIndexer, OneHotEncoder)\nstring_indexer=[ StringIndexer(inputCol=x, outputCol=x+'_StringIndexer', handleInvalid='skip') for x in str_columns]\none_hot_encoder=[OneHotEncoder(inputCol=f\"{x}_StringIndexer\", outputCol=f\"{x}_OneHotEncoder\") for x in str_columns]\n\nfrom pyspark.ml.feature import VectorAssembler\nassembler_input=[x for x in num_columns]\nassembler_input+=[f\"{x}_OneHotEncoder\" for x in str_columns]\nvector_assembler=VectorAssembler(inputCols=assembler_input, outputCol='features')\n\ntrain, test= serial_df.randomSplit([0.7,0.3],seed=7)\n\nstages=[]\nstages+=string_indexer\nstages+=one_hot_encoder\nstages+=[vector_assembler]\n\nfrom pyspark.ml import Pipeline\npipeline=Pipeline().setStages(stages)\nmodel=pipeline.fit(train)\nX_train=model.transform(train)\nX_train.createOrReplaceTempView(\"X_train\")\ndata=sqlContext.sql(\"select distinct(features),label from X_train\")\n\nfrom pyspark.ml.classification import LogisticRegression\nlr=LogisticRegression().fit(data)\nprint('train_auc={}'.format(lr.summary.areaUnderROC))\n\nX_test=model.transform(test)\npredicts=lr.transform(X_test)\n\nscores=np.array(predicts.select('probability').collect())\nlbls=np.array(predicts.select('label').collect())\npreds=np.array(predicts.select('prediction').collect())\nscores_1=[]\nfor score in scores:\n    scores_1.append(score[0][1])\n    \nimport sklearn.metrics as metrics\nfpr, tpr, threshold = metrics.roc_curve(lbls, scores_1)\nroc_auc = metrics.auc(fpr, tpr)\n\nimport matplotlib.pyplot as plt\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\n\nconf_mat=metrics.confusion_matrix(lbls,preds)\naccuracy=metrics.accuracy_score(lbls,preds)\nprint('test_auc={}\\ntest_accuracy={}\\nconfision_mat={}'.format(roc_auc,accuracy,conf_mat))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **4. Conclusions**","metadata":{}},{"cell_type":"markdown","source":"# **5. Discussion**","metadata":{}},{"cell_type":"markdown","source":"In this project, as disscussed we were able to successfully implement a ML algorithm on a Spark framework. However, there were many things which we were unable to achieve due to various constrains. Some ideas which were thought out during the project but we were unable to perform include-\n\n* Determining the signal and background data in a better way by performing some statistical tests like Chi Square test etc.\n\n* Doing more rigorous feature selection by performing more traditional Data selection processes.\n\n* Implementing more complex ML algorithm","metadata":{}},{"cell_type":"markdown","source":"# **6. References**","metadata":{"execution":{"iopub.execute_input":"2021-09-05T11:12:57.481636Z","iopub.status.busy":"2021-09-05T11:12:57.481187Z","iopub.status.idle":"2021-09-05T11:12:57.486368Z","shell.execute_reply":"2021-09-05T11:12:57.485384Z","shell.execute_reply.started":"2021-09-05T11:12:57.481538Z"}}},{"cell_type":"markdown","source":"1. Logistic regression: https://spark.apache.org/docs/latest/mllib-linear-methods.html#logistic-regression; https://www.pdfdrive.com/applied-logistic-regression-e172207141.html\n\n2. Apache Spark documentation: https://spark.apache.org/\n\n3. Apache Spark blog by AWS: https://aws.amazon.com/big-data/what-is-spark/\n\n4. Machine Learning Definition: https://mitsloan.mit.edu/ideas-made-to-matter/machine-learning-explained\n\n5. ML types: https://www.geeksforgeeks.org/introduction-machine-learning/\n\n6. \n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}