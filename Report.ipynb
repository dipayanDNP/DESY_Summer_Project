{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Motivation**\n",
    "As in each system, breakdowns and errors sometimes occur in **dCahce**. Few people would dispute that it is important to detect them automatically and warn users afterward. Imagine the situation, a scientist from Japan failed to access data from the node placed in the mid-Europian region, and now he is trying to figure out if there was a problem with his local machine or with the entire system, but he can not get any information, because due to the difference in time zones local office in Europe is closed already. In such cases, it is especially important to warn the user if something was wrong. That is why we decided to develop a **machine learning** model, helping to determine undesirable situations.\n",
    "  \n",
    "This report is devoted to our solution for this problem. We have prepared the model, analysing information about dCache transactions and  distinguishing `signal` - when the system works fine, and `backgorund` - when something goes wrong. You can find a more detailed description on the following pages of the report.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Introduction**\n",
    "## **1.1. dCache**\n",
    "   First of all, it worths saying some words about the system we are going to work with -  dCache. Built in Java dCache is a distributed mass-storage system that allows us to manage huge ammount of scientific data. The data are distributed among the large number of heterogenous pools(nodes) that handle with data storage and transfer. A client can easily get access to dCache data through requests.  \n",
    "   \n",
    "   Information we are intersted in is about transactions occurred in dCache. It is contained in *billing* files which are a set of JSON dictionaries for a particular date. There are several main types of transactions: requset, transfer, remove, store, restore, but  only `stores` will be in our sphere of interests (later will be explained why).\n",
    "\n",
    "   Since amount of data to process is huge enough, to avoid overloading of our local machines we use unified analytics engine for large-scale data processing - **Apache Spark**.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.2. Apache Spark**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apache Spark** is an open-source, distributed processing system used for big data workloads. It utilizes in-memory caching, and optimized query execution for fast analytic queries against data of any size. [1.]\n",
    "\n",
    "Spark was introduced in 2012. Before Spark, Hadoop MapReduce was commonly used for big data analytics. \n",
    "\n",
    "Hadoop MapReduce processes big datasets with a parallel, distributed algorithm. However, a challenge in using MapReduce is the sequential multi-step process it takes to run a job. With each step requiring a disk read, and write, MapReduce jobs are slower due to the latency of disk I/O.\n",
    "\n",
    "To overcome this problem, Spark was created. It achieves this by processing in-memory, reducing the number of steps in a job, and by reusing data across multiple parallel operations. It is accomplished through the creation of DataFrames, an abstraction over Resilient Distributed Dataset (RDD), which is a collection of objects that is cached in memory, and reused in multiple Spark operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.3. Machine Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Machine learning(ML)** is a subfield of artificial intelligence, which is broadly defined as the capability of a machine to imitate intelligent human behavior. Artificial intelligence systems are used to perform complex tasks in a way that is similar to how humans solve problems.  [2.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadly, there are mainly 3 types of ML algorithms:\n",
    "\n",
    "* Supervised Learning: When an algorithm learns from example data and associated target responses that can consist of numeric values or string labels, such as classes or tags, in order to later predict the correct response when posed with new examples comes under the category of Supervised learning.\n",
    "\n",
    "* Unsupervised learning: When an algorithm learns from plain examples without any associated response, leaving to the algorithm to determine the data patterns on its own.\n",
    "\n",
    "* Reinforcement learning: When you present the algorithm with examples that lack labels, as in unsupervised learning. However, you can accompany an example with positive or negative feedback according to the solution so that the algorithm makes its own decisions, and the decisions bear consequences.\n",
    "\n",
    "<a href=\"https://ibb.co/sH5Nv38\"><img src=\"https://i.ibb.co/YZD17jH/figure01.png\" alt=\"figure01\" border=\"0\"></a>\n",
    "\n",
    "Source: [Link](https://developer.ibm.com/articles/cc-models-machine-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.4. Logistic Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Reression is a type of Supervised Algorithm that is used for classification problems, i.e. correctly classifying various data points to their correct data labels. It calculates the probability of the datasample being a particular class. \n",
    "\n",
    "In our project, we have worked with binary class Logistic Regression Model which classifies 2 Labels 0 and 1.\n",
    "\n",
    "The conditional probability that our Logistic model gives a particular class given the dataset is given by -\n",
    "\n",
    "<a href=\"https://ibb.co/vXSL8Xr\"><img src=\"https://i.ibb.co/XYqFTYd/1-I0l-W7-Ydv-Tn3m-HXh56p-Yx-ZQ.gif\" alt=\"1-I0l-W7-Ydv-Tn3m-HXh56p-Yx-ZQ\" border=\"0\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where\n",
    "\n",
    "* w = weight values which are determined by our ML algorithm\n",
    "\n",
    "* x = data input\n",
    "\n",
    "* y = conditional probability of predicting a particular class given the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of y with a one dimensional data input x will have the following form - \n",
    "\n",
    "<a href=\"https://ibb.co/sg1spY7\"><img src=\"https://i.ibb.co/GdMCfSm/1-Un-SW1b5-Ldp-Fl-Bx5h-R54-J0w.png\" alt=\"1-Un-SW1b5-Ldp-Fl-Bx5h-R54-J0w\" border=\"0\"></a>\n",
    "\n",
    "Source: [Link](https://towardsdatascience.com/an-introduction-to-logistic-regression-8136ad65da2e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Analysis**\n",
    "## **2.1. Importing Libraries and setting up the Spark Configuration**\n",
    "The following libraries and Spark Configurations were used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import os\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark_dist_explore import hist\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import time\n",
    "import re\n",
    "from ipaddress import ip_address\n",
    "from urllib.parse import urlparse\n",
    "import json\n",
    "import isodate\n",
    "from dateutil import parser\n",
    "from dateutil import tz\n",
    "from pyspark.sql.types import StructField, StructType, StringType\n",
    "from pyspark.sql import Row\n",
    "from urllib.request import urlopen\n",
    "\n",
    "sc_conf = SparkConf().setMaster('spark://dcache-dot1.desy.de:5000').set('spark.executor.memory',\n",
    "'26G').set('spark.driver.memory','8G').set('spark.driver.maxResultSize','8G')\n",
    "#sc = SparkContext(appName=\"PythonStreaming\")\n",
    "sc = SparkContext(conf = sc_conf)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.2. Data Pre-Selection**\n",
    "As was said before, we were interested in only `store` type of transactions. There are two main reasons for it. First - structure of messages with type `store` is not really complicated, there are not many features to analyze in comparison with `transfer`, for instance. Second - there are sufficient number of instances with message type `store` in dCache.\n",
    "\n",
    "Two random days were chosen for analysis: 2021-07-10 and 2021-08-01. For converting data to RDD we wrote special function `convert_data`. As you may see, there are two parametrs in it: `file` - a file's directory, `msgType` - type of a message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data(file, msgType):\n",
    "    data = sc.textFile(file)\n",
    "    billing = data.map(lambda row: json.loads(row)).filter(lambda row: row.get('msgType',None) == msgType)\n",
    "    return billing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To combine data from both days SparkContext method *union()* was used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "billing_RDD = sc.union(\n",
    "    [\n",
    "        convert_data('/pnfs/desy.de/desy/dcache-operations/billing-archive/xfel/2021/07/billing-2021-07-10.json',\"store\"),\n",
    "        convert_data('/pnfs/desy.de/desy/dcache-operations/billing-archive/xfel/2021/08/billing-2021-08-01.json',\"store\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.3. Feature Description**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After selecting the msgType = \"Store\", we can see that the RDD has many columns of data. \n",
    "\n",
    "All the columns along with an example, can be seen using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "billing_RDD.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output:\n",
    "{'transferTime': 23,\n",
    " 'version': '1.0',\n",
    " 'date': '2021-07-10T02:01:27.432+02:00',\n",
    " 'cellDomain': 'dcache-xfel132-06Domain',\n",
    " 'pnfsid': '00005F97A5611EE74DE68C4EFB82451E8ADD',\n",
    " 'transaction': 'pool:dcache-xfel132-06@dcache-xfel132-06Domain:1625875287432-12713',\n",
    " 'status': {'code': 66, 'msg': 'HSM script failed (script reported: 66: )'},\n",
    " 'msgType': 'store',\n",
    " 'fileSize': 9514620780,\n",
    " 'billingPath': '/pnfs/desy.de/exfel/archive/XFEL/proc//SPB/201701/p002038/r0220/CORR-R0220-AGIPD04-S00003.h5',\n",
    " 'storageInfo': 'xfel:SPB-201701PROC@osm2',\n",
    " 'queuingTime': 0,\n",
    " '@version': '1',\n",
    " '@timestamp': '2021-07-10T00:01:27.457Z',\n",
    " 'tags': ['xfel'],\n",
    " 'cellType': 'pool',\n",
    " 'session': 'pool:dcache-xfel132-06@dcache-xfel132-06Domain:1625875287432-12713',\n",
    " 'cellName': 'dcache-xfel132-06'}    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, all this columns are not required for our ML analysis. So, we have selected few columns which we deemed important for our task of Anomaly Detection.\n",
    "\n",
    "All the other columns were rejected because they were unique label to each event and thus wouldn't provide much insight into the Anomaly Detection algorithm that we are trying to construct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Features | Description |\n",
    "|----------|-------------|\n",
    "| CellName |             |\n",
    "|          |             |\n",
    "|          |             |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.4. Data Pre-Processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before the data can be used for our ML purposes it has to be transformed so that they are suitable for applying our ML algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use some wrapper functions to do some initial transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Queueing Time - No modification **(with modifications, isn't it?)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cellName(cellName):\n",
    "    s=cellName[len(cellName)-5:]\n",
    "    s1=s.split('-')\n",
    "    return str(s1[0])+str(s1[1])\n",
    "\n",
    "def date_time(date):\n",
    "    return int(time.mktime(parser.parse(date).timetuple()))\n",
    "\n",
    "def fileSize(fileSize):\n",
    "    return float(np.log(fileSize))\n",
    "\n",
    "def queuingTime(queuingTime):\n",
    "    if (queuingTime == 0) | (queuingTime is None):\n",
    "        return -10.0\n",
    "    else:    \n",
    "        return float(np.log(queuingTime))\n",
    "    \n",
    "def transferTime(transferTime):\n",
    "    return float(np.log(transferTime))\n",
    "\n",
    "def initial_label(status):\n",
    "    if status['code']!=0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "billing_Schema =  StructType([\n",
    "    StructField('cellName', StringType(), True),\n",
    "    StructField('date_time', LongType(), True),\n",
    "    StructField('fileSize', FloatType(), True),\n",
    "    StructField('transferTime', FloatType(), True),\n",
    "    StructField('queuingTime', FloatType(), True),\n",
    "    StructField('initial_label', LongType(), True)\n",
    "   ])\n",
    "\n",
    "def parse_billing(entry):\n",
    "    parse_list = [ cellName(entry.get('cellName')),\n",
    "                   date_time(entry.get('date')),\n",
    "                   fileSize(entry.get('fileSize')),\n",
    "                   transferTime(entry.get('transferTime')),\n",
    "                   queuingTime(entry.get('queuingTime')),\n",
    "                   initial_label(entry.get('status'))\n",
    "                  ]\n",
    "    info = tuple( field for field in parse_list )\n",
    "    return info\n",
    "\n",
    "billing_df = sqlContext.createDataFrame(billing_RDD.map(lambda s: parse_billing(s)), billing_Schema)\n",
    "billing_df.createOrReplaceTempView(\"billing_desy\") #name creation for sql quiries\n",
    "billing_df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have one column `cellName` which has 'String' type. Since, a 'String' type column can't directly be used for any ML analysis, we have to convert it into a suitable form.\n",
    "\n",
    "We use the `StringIndexer` module in Mllib followed by the `OneHotEncoder` module, which transforms the CellName columns into a sparse matrix which can then be used for ML analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "str_columns=['cellName']\n",
    "string_indexer=[ StringIndexer(inputCol=x, outputCol=x+'_StringIndexer', handleInvalid='skip') for x in str_columns]\n",
    "one_hot_encoder=[OneHotEncoder(inputCol=f\"{x}_StringIndexer\", outputCol=f\"{x}_OneHotEncoder\") for x in str_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to apply Logistic Regression on our data we have to convert the DataFrame into a single vector of features called 'features' which is achieved by the `Vectorizer` module in Mllib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "num_columns=['fileSize', 'transferTime', 'queuingTime']\n",
    "assembler_input=num_columns+[f\"{x}_OneHotEncoder\" for x in str_columns]\n",
    "vector_assembler=VectorAssembler(inputCols=assembler_input, outputCol='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that `date_time` field was not included in the vector of features. That is because `date_time` is a unique feature for each file, which does not provide an ML model with important information. However, it plays a crucial role in data selection (later will be shown how), that is why we did not exclude `date_time` from the list of fields. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.5. Data Sampling and Labeling**\n",
    "Since, we had 2 classes to distinguish - `signal` and `background`, we had to use a classification model for our problem.\n",
    "\n",
    "Before starting to apply machine learning for classifictaion, it is essetial to prepare data samples and label instances correctly. \n",
    "\n",
    "First of all, we would like to describe labeling of the data instances. Initial approach with using `status_code` feature (look at the `initial_label` field, point 2.5) is a bit naive to make a decision if a particular instance is a part of background or signal. So, we have developed more sophisticated algorithm. New labeling algorithm includes two steps:\n",
    "* Using `date_time` field, we divide all available time (for two chosen dates) on a particular number of bins, each bin equals one hour of time. \n",
    "* Averaging values of the field `initial_label` for a particular bin, we compare output value with the threshold (threshold=0.5 in our case). If the averaged value is less than the threshold we consider all instances as signal (fill 0 as the `label` for each instance of the bin), otherwise as background (fill 1 as the the `label` for each instance of the bin).\n",
    "\n",
    "The second issue is how to sample the data. For comparison we decided to apply two ways:\n",
    "* Extracting first 70 % instances for each hour sequently (serial way) \n",
    "* Extracting  70 % instances for each hour randomly (random way)\n",
    "\n",
    "Code examples, including labeling and two ways of the data selection, are represented below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"serial way\"\"\"\n",
    "\n",
    "#import lit for labeling\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "#creation empty dataframe for followed union\n",
    "billing_Schema_empty =  StructType([\n",
    "    StructField('cellName', StringType(), True),\n",
    "    StructField('date_time', LongType(), True),\n",
    "    StructField('fileSize', FloatType(), True),\n",
    "    StructField('transferTime', FloatType(), True),\n",
    "    StructField('queuingTime', FloatType(), True),\n",
    "    StructField('initial_label', LongType(), True),\n",
    "    StructField('label', IntegerType(), True)\n",
    "   ])\n",
    "serial_df=sqlContext.createDataFrame(sc.emptyRDD(), billing_Schema_empty)\n",
    "    \n",
    "for i in range(int(bin_num)):\n",
    "    #spliting by hours\n",
    "    new_df=sqlContext.sql(\"select * from billing_desy where date_time between {} and {} order by date_time DESC\".format(\n",
    "        int(min_val)+3600*i,int(min_val)+3600*(i+1)))\n",
    "    new_avg=sqlContext.sql(\"select AVG(initial_label) from billing_desy where date_time between {} and {}\".format(\n",
    "        int(min_val)+3600*i,int(min_val)+3600*(i+1))).collect()\n",
    "    new_avg=np.array(new_avg)[0][0]\n",
    "    \n",
    "    if new_avg is None:\n",
    "        continue\n",
    "        \n",
    "    else:\n",
    "        new_avg=float(new_avg)\n",
    "        \n",
    "        #serial extracting\n",
    "        per_70=new_df.take(int(np.round(0.7*new_df.count(),0))+1)\n",
    "        per_70_df=sqlContext.createDataFrame(per_70)\n",
    "    \n",
    "    #labeling\n",
    "    if new_avg>=treshhold:\n",
    "        per_70_df=per_70_df.withColumn(\"label\", lit(1))\n",
    "    else:\n",
    "        per_70_df=per_70_df.withColumn(\"label\", lit(0))\n",
    "    serial_df=serial_df.union(per_70_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"random way\"\"\"\n",
    "\n",
    "#creation of empty dataframe for followed union\n",
    "billing_Schema_empty =  StructType([\n",
    "    StructField('cellName', StringType(), True),\n",
    "    StructField('date_time', LongType(), True),\n",
    "    StructField('fileSize', FloatType(), True),\n",
    "    StructField('transferTime', FloatType(), True),\n",
    "    StructField('queuingTime', FloatType(), True),\n",
    "    StructField('initial_label', LongType(), True),\n",
    "    StructField('label', IntegerType(), True)\n",
    "   ])\n",
    "rand_df=sqlContext.createDataFrame(sc.emptyRDD(), billing_Schema_empty)\n",
    "    \n",
    "for i in range(int(bin_num)):\n",
    "    #spliting by hours\n",
    "    new_df=sqlContext.sql(\"select * from billing_desy where date_time between {} and {} order by date_time DESC\".format(\n",
    "        int(min_val)+3600*i,int(min_val)+3600*(i+1)))\n",
    "    new_avg=sqlContext.sql(\"select AVG(initial_label) from billing_desy where date_time between {} and {}\".format(\n",
    "        int(min_val)+3600*i,int(min_val)+3600*(i+1))).collect()\n",
    "    new_avg=np.array(new_avg)[0][0]\n",
    "    \n",
    "    if new_avg is None:\n",
    "        continue\n",
    "        \n",
    "    else:\n",
    "        new_avg=float(new_avg)\n",
    "        \n",
    "        #random extracting\n",
    "        rand_70_df, rand_30_df= new_df.randomSplit([0.7,0.3],seed=7)\n",
    "        \n",
    "    #labeling\n",
    "    if new_avg>=treshhold:\n",
    "        rand_70_df=rand_70_df.withColumn(\"label\", lit(1))\n",
    "    else:\n",
    "        rand_70_df=rand_70_df.withColumn(\"label\", lit(0))\n",
    "        \n",
    "    rand_df=rand_df.union(rand_70_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.6. Algorithm Selection**\n",
    "Having prepared data, chosen samples and labeled instances, we are able to apply machine learning algorithms. To simplify things and easily get decision probabilities, we chose `Logistic Regression` as our Classifier model. We used the `Mllib` module that is availaible with spark to perform our analysis.\n",
    "\n",
    "To combine all steps of the data pre-processing (look at point 2.5) and not to do them separately each time, we used `Pipline`. Output data of `Pipline` model are appropriate for the training of `Logistic Regression` model. Code goes below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting of selected data on train and test samples (serial_df for sequential selection is in this exaple, rand_df is also possible)\n",
    "train, test= serial_df.randomSplit([0.7,0.3],seed=7)\n",
    "\n",
    "#designation of stages for Pipline\n",
    "stages=[]\n",
    "stages+=string_indexer\n",
    "stages+=one_hot_encoder\n",
    "stages+=[vector_assembler]\n",
    "\n",
    "#Pipline model creation\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline=Pipeline().setStages(stages)\n",
    "model=pipeline.fit(train)\n",
    "\n",
    "#transforming of the train sample for Logistic Regression\n",
    "X_train=model.transform(train)\n",
    "X_train.createOrReplaceTempView(\"X_train\")\n",
    "data=sqlContext.sql(\"select distinct(features),label from X_train\")\n",
    "\n",
    "#training of Logistic Regression model\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr=LogisticRegression().fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-05T11:12:57.481636Z",
     "iopub.status.busy": "2021-09-05T11:12:57.481187Z",
     "iopub.status.idle": "2021-09-05T11:12:57.486368Z",
     "shell.execute_reply": "2021-09-05T11:12:57.485384Z",
     "shell.execute_reply.started": "2021-09-05T11:12:57.481538Z"
    }
   },
   "source": [
    "# **4. References**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Logistic regression: https://spark.apache.org/docs/latest/mllib-linear-methods.html#logistic-regression; https://www.pdfdrive.com/applied-logistic-regression-e172207141.html\n",
    "\n",
    "2. Apache Spark documentation: https://spark.apache.org/\n",
    "\n",
    "3. Apache Spark blog by AWS: https://aws.amazon.com/big-data/what-is-spark/\n",
    "\n",
    "4. Machine Learning Definition: https://mitsloan.mit.edu/ideas-made-to-matter/machine-learning-explained\n",
    "\n",
    "5. ML types: https://www.geeksforgeeks.org/introduction-machine-learning/\n",
    "\n",
    "6. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
